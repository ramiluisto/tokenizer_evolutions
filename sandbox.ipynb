{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:05<00:00, 19.65it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_paths_folder = Path('./tokenizers')\n",
    "subfolders = [f for f in tokenizer_paths_folder.iterdir() if f.is_dir() and f.name.startswith('size_variation_')]\n",
    "\n",
    "vocabularies = []\n",
    "for subfolder in tqdm(subfolders):\n",
    "    tokenizer_path = subfolder / 'tokenizer.json'\n",
    "    if tokenizer_path.exists():\n",
    "        with open(tokenizer_path, 'r') as f:\n",
    "            tokenizer_data = json.load(f)\n",
    "        tokens = tokenizer_data['model']['vocab']\n",
    "        vocabularies.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 172.94it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_size_dict = {}\n",
    "raw_vocab_size_dict = {}\n",
    "vocab_sizes = []\n",
    "for vocab in tqdm(vocabularies):\n",
    "    vocab_size = len(vocab)\n",
    "    vocab_sizes.append(vocab_size)\n",
    "    token_lengths = [len(token) for token in vocab.keys() if len(token) <= 20]\n",
    "    raw_token_lengths = [len(token) for token in vocab.keys()]\n",
    "    vocab_size_dict[vocab_size] = token_lengths\n",
    "    raw_vocab_size_dict[vocab_size] = raw_token_lengths\n",
    "# sort vocab_size_dict by key\n",
    "vocab_size_dict = dict(sorted(vocab_size_dict.items()))\n",
    "raw_vocab_size_dict = dict(sorted(raw_vocab_size_dict.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:14<00:00,  5.58it/s]\n",
      "100%|██████████| 83/83 [00:01<00:00, 50.60it/s] \n",
      "100%|██████████| 83/83 [00:03<00:00, 26.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for vocab_len, token_lengths in tqdm(vocab_size_dict.items()):\n",
    "    if vocab_len == 1000:\n",
    "        sns.kdeplot(token_lengths, label=vocab_len, bw_method='silverman', linewidth=2)\n",
    "    else:\n",
    "        sns.kdeplot(token_lengths, label=vocab_len, bw_method='silverman')\n",
    "\n",
    "plt.title(f\"Distribution of token lengths across vocab sizes\")\n",
    "plt.xlabel(\"Token length\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(ncol=3)\n",
    "plt.xticks(range(0, 21, 1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"images\", f\"length_distribution_full.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for vocab_len, token_lengths in tqdm(vocab_size_dict.items()):\n",
    "    if vocab_len in range(1000, 100000, 10000):\n",
    "        sns.kdeplot(token_lengths, label=vocab_len, bw_method='silverman')\n",
    "\n",
    "plt.title(f\"Distribution of token lengths across vocab sizes\")\n",
    "plt.xlabel(\"Token length\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(ncol=3)\n",
    "plt.xticks(range(0, 21, 1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"images\", f\"length_distribution_every_tenth.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for vocab_len, token_lengths in tqdm(vocab_size_dict.items()):\n",
    "    if vocab_len in range(1000, 50000, 2000):\n",
    "        sns.kdeplot(token_lengths, label=vocab_len, bw_method='silverman')\n",
    "\n",
    "plt.title(f\"Distribution of token lengths across vocab sizes\")\n",
    "plt.xlabel(\"Token length\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(ncol=3)\n",
    "plt.xticks(range(0, 21, 1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"images\", f\"length_distribution_start.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for each vocabulary size\n",
    "stats = {\n",
    "    'max': [],\n",
    "    'mean': [],\n",
    "    'median': [],\n",
    "    '90th_percentile': []\n",
    "}\n",
    "vocab_sizes = []\n",
    "\n",
    "for vocab_len, token_lengths in raw_vocab_size_dict.items():\n",
    "    vocab_sizes.append(vocab_len)\n",
    "    stats['max'].append(max(token_lengths))\n",
    "    stats['mean'].append(np.mean(token_lengths))\n",
    "    stats['median'].append(np.median(token_lengths))\n",
    "    stats['90th_percentile'].append(np.percentile(token_lengths, 90))\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for stat_name, values in stats.items():\n",
    "    plt.plot(vocab_sizes, values, label=stat_name, marker='o', markersize=4)\n",
    "\n",
    "plt.title(\"Token Length Statistics Across Vocabulary Sizes\")\n",
    "plt.xlabel(\"Vocabulary Size\")\n",
    "plt.ylabel(\"Token Length\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"images\", \"token_length_statistics.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# suppose token_lengths is a list of ints\n",
    "counts = Counter(token_lengths)\n",
    "xs = np.array(sorted(counts))         # unique token lengths\n",
    "ys = np.array([counts[x] for x in xs])  # raw counts\n",
    "\n",
    "# if you want a density instead of raw counts:\n",
    "ys = ys / ys.sum()                    # now sums to 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "smallest_vocab_len = min(vocab_size_dict.keys())\n",
    "biggest_vocab_len = max(vocab_size_dict.keys())\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for vocab_len, token_lengths in vocab_size_dict.items():\n",
    "    # build (x,y)\n",
    "    counts = Counter(token_lengths)\n",
    "    xs = np.array(sorted(counts))\n",
    "    ys = np.array([counts[x] for x in xs]) / sum(counts.values())\n",
    "\n",
    "    # piecewise linear line\n",
    "    if vocab_len in [smallest_vocab_len, biggest_vocab_len]:\n",
    "        plt.plot(xs, ys, label=f\"Vocab {vocab_len}\", linewidth=2)\n",
    "    else:\n",
    "        plt.plot(xs, ys, linewidth=1)\n",
    "\n",
    "plt.title(f\"Distribution of token lengths across vocab sizes\")\n",
    "plt.xlabel(\"Token length\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.xticks(range(0, 21, 1))\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(\"images\", f\"length_distribution_jagged.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# suppose token_lengths is a list of ints\n",
    "counts = Counter(token_lengths)\n",
    "xs = np.array(sorted(counts))         # unique token lengths\n",
    "ys = np.array([counts[x] for x in xs])  # raw counts\n",
    "\n",
    "# if you want a density instead of raw counts:\n",
    "ys = ys / ys.sum()                    # now sums to 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for vocab_len, token_lengths in vocab_size_dict.items():\n",
    "\n",
    "    if vocab_len in range(1000, 10000, 1000):\n",
    "        # build (x,y)\n",
    "        counts = Counter(token_lengths)\n",
    "        xs = np.array(sorted(counts))\n",
    "        ys = np.array([counts[x] for x in xs]) / sum(counts.values())\n",
    "\n",
    "        # piecewise linear line\n",
    "\n",
    "        plt.plot(xs, ys, label=f\"Vocab {vocab_len}\", linewidth=1)\n",
    "\n",
    "plt.title(f\"Distribution of token lengths across vocab sizes\")\n",
    "plt.xlabel(\"Token length\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(ncol=3)\n",
    "plt.xticks(range(0, 21, 1))\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(\"images\", f\"length_distribution_jagged_start.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side-by-side plots created and saved to 'images/token_length_statistics_comparison.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Calculate statistics for each vocabulary size\n",
    "stats = {\n",
    "    'max': [],\n",
    "    'mean': [],\n",
    "    'median': [],\n",
    "    '90th_percentile': []\n",
    "}\n",
    "vocab_sizes = []\n",
    "\n",
    "for vocab_len, token_lengths in raw_vocab_size_dict.items():\n",
    "    vocab_sizes.append(vocab_len)\n",
    "    stats['max'].append(max(token_lengths))\n",
    "    stats['mean'].append(np.mean(token_lengths))\n",
    "    stats['median'].append(np.median(token_lengths))\n",
    "    stats['90th_percentile'].append(np.percentile(token_lengths, 90))\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharey=False)\n",
    "\n",
    "# First subplot with all statistics (including max)\n",
    "for stat_name, values in stats.items():\n",
    "    ax1.plot(vocab_sizes, values, label=stat_name, marker='o', markersize=4)\n",
    "\n",
    "ax1.set_title(\"All Token Length Statistics\")\n",
    "ax1.set_xlabel(\"Vocabulary Size\")\n",
    "ax1.set_ylabel(\"Token Length\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Second subplot with all statistics except max\n",
    "for stat_name, values in stats.items():\n",
    "    if stat_name != 'max':  # Skip the max statistic for the second plot\n",
    "        ax2.plot(vocab_sizes, values, label=stat_name, marker='o', markersize=4)\n",
    "\n",
    "ax2.set_title(\"Token Length Statistics (without max)\")\n",
    "ax2.set_xlabel(\"Vocabulary Size\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Ensure images directory exists\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"images\", \"token_length_statistics_comparison.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"Side-by-side plots created and saved to 'images/token_length_statistics_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean statistic for each vocabulary size\n",
    "mean_values = []\n",
    "vocab_sizes = []\n",
    "\n",
    "for vocab_len, token_lengths in raw_vocab_size_dict.items():\n",
    "    vocab_sizes.append(vocab_len)\n",
    "    mean_values.append(np.mean(token_lengths))\n",
    "\n",
    "# Create a single plot with log scale for y-axis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(vocab_sizes, mean_values, label='Mean Token Length', marker='o', markersize=4, color='blue')\n",
    "\n",
    "plt.title(\"Mean Token Length Across Vocabulary Sizes\")\n",
    "plt.xlabel(\"Vocabulary Size (log scale\")\n",
    "plt.ylabel(\"Mean Token Length)\")\n",
    "plt.xscale('log')  # Set y-axis to log scale\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\")  # Grid lines for both major and minor ticks\n",
    "\n",
    "# Ensure images directory exists\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"images\", \"mean_token_length_log_scale.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocGen312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
